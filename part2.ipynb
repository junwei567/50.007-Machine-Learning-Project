{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_TRAIN_DATA_FILE = \"./ES/train\"\n",
    "ES_TEST_DATA_FILE = \"./ES/dev.in\"\n",
    "ES_ACTUAL_DATA_FILE = \"./ES/dev.out\"\n",
    "ES_PART1_PREDICTED_DATA_FILE = \"./ES/dev.p1.out\"\n",
    "ES_PART2_PREDICTED_DATA_FILE = \"./ES/dev.p2.out\"\n",
    "ES_PART3_PREDICTED_DATA_FILE = \"./ES/dev.p3.out\"\n",
    "ES_PART4_PREDICTED_DATA_FILE = \"./ES/dev.p4.out\"\n",
    "\n",
    "RU_TRAIN_DATA_FILE = \"./RU/train\"\n",
    "RU_TEST_DATA_FILE = \"./RU/dev.in\"\n",
    "RU_ACTUAL_DATA_FILE = \"./RU/dev.out\"\n",
    "RU_PART1_PREDICTED_DATA_FILE = \"./RU/dev.p1.out\"\n",
    "RU_PART2_PREDICTED_DATA_FILE = \"./RU/dev.p2.out\"\n",
    "RU_PART3_PREDICTED_DATA_FILE = \"./RU/dev.p3.out\"\n",
    "RU_PART4_PREDICTED_DATA_FILE = \"./RU/dev.p4.out\"\n",
    "\n",
    "UNK_WORD = \"#UNK#\"\n",
    "START_TAG = \"START\"\n",
    "STOP_TAG = \"STOP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def _preprocess_training_file(training_file):\n",
    "    tags = []\n",
    "    tags_with_start_stop = []\n",
    "    words = []\n",
    "\n",
    "    with open(training_file,\"r\",encoding=\"utf8\") as f:\n",
    "        document = f.read().rstrip()\n",
    "        lines = document.split(\"\\n\\n\")\n",
    "\n",
    "        for line in lines:\n",
    "            tags_list = []\n",
    "            tags_with_start_stop_list = []\n",
    "            words_list = []\n",
    "\n",
    "            for word_tag in line.split(\"\\n\"):\n",
    "                i = word_tag.split(\" \")\n",
    "\n",
    "                if len(i) > 2:\n",
    "                    i = [\" \".join(i[0:len(i)-1]), i[len(i)-1]]\n",
    "\n",
    "                word, tag = i[0], i[1]\n",
    "                words_list.append(word)\n",
    "                tags_list.append(tag)\n",
    "\n",
    "            tags.append(tags_list)\n",
    "            tags_with_start_stop_list = [START_TAG] + tags_list + [STOP_TAG]\n",
    "            tags_with_start_stop.append(tags_with_start_stop_list)\n",
    "            words.append(words_list)\n",
    "    \n",
    "    return tags, tags_with_start_stop, words\n",
    "\n",
    "def _preprocess_test_file(testing_file):\n",
    "    test_words = []\n",
    "\n",
    "    with open(testing_file,\"r\",encoding=\"utf8\") as f:\n",
    "        document = f.read().rstrip()\n",
    "        lines = document.split(\"\\n\\n\")\n",
    "\n",
    "        for line in lines:\n",
    "            word_list = []\n",
    "            for word in line.split(\"\\n\"):\n",
    "                word_list.append(word)\n",
    "            test_words.append(word_list)\n",
    "\n",
    "    return test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_pairs(tags):\n",
    "    transition_pair_count = {}\n",
    "\n",
    "    for tag in tags:\n",
    "        for tag1, tag2 in zip(tag[:-1], tag[1:]):\n",
    "            transition_pair = (tag1, tag2)\n",
    "            if transition_pair in transition_pair_count:\n",
    "                transition_pair_count[transition_pair] += 1\n",
    "            else:\n",
    "                transition_pair_count[transition_pair] = 1\n",
    "\n",
    "    return transition_pair_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique(x):\n",
    "    return list(set(list(itertools.chain.from_iterable(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_y(tag, tags):\n",
    "    tags_flattened = list(itertools.chain.from_iterable(tags))\n",
    "    return tags_flattened.count(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emission_using_MLE(unique_tags, unique_words, tags, words, k=1):\n",
    "    emission = {}\n",
    "    unique_tags.extend([START_TAG, STOP_TAG])\n",
    "\n",
    "    for tag in unique_tags:\n",
    "        emission_word = {}\n",
    "        for word in unique_words:\n",
    "            emission_word[word] = 0.0\n",
    "        emission_word[UNK_WORD] = 0.0\n",
    "        emission[tag] = emission_word\n",
    "\n",
    "    # fill up emission dictionary\n",
    "    for tag_sentence, word_sentence in zip(tags, words):\n",
    "        for tag, word in zip(tag_sentence, word_sentence):\n",
    "            emission[tag][word] += 1\n",
    "            \n",
    "    for tag, emission_word in emission.items():\n",
    "        tag_count = count_y(tag, tags) + k\n",
    "        for word, count_y_x in emission_word.items():\n",
    "            emission[tag][word] = count_y_x / (tag_count)\n",
    "\n",
    "        emission[tag][UNK_WORD] = k / (tag_count)\n",
    "        \n",
    "    return emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_using_MLE(unique_tags, tags_with_start_stop, transition_pair_count):\n",
    "    unique_tags = [START_TAG] + unique_tags + [STOP_TAG]\n",
    "    transition = {}\n",
    "    \n",
    "    for u in unique_tags[:-1]: # omit STOP\n",
    "        transition_tag = {}\n",
    "        for v in unique_tags[1:]: # omit START\n",
    "            transition_tag[v] = 0.0\n",
    "        transition[u] = transition_tag\n",
    "\n",
    "    # fill up transition parameters\n",
    "    for u, v in transition_pair_count:\n",
    "        transition[u][v] += transition_pair_count[(u, v)]\n",
    "    \n",
    "    # divide transition_count by count_yi, to get probability\n",
    "    for u, transition_tag in transition.items():\n",
    "        count_yi = count_y(u, tags_with_start_stop)\n",
    "        # words in training set\n",
    "        for v, transition_count in transition_tag.items():\n",
    "            if count_yi == 0:\n",
    "                transition[u][v] = 0.0\n",
    "            else:\n",
    "                transition[u][v] = transition_count / count_yi\n",
    "\n",
    "    return transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unknown_words(train, test):\n",
    "    test_words = get_unique(test)\n",
    "    train_words = get_unique(train)\n",
    "    return set(test_words) - set(train_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess for ES Dataset\n",
    "ES_tags, ES_tags_with_start_stop, ES_train_words = _preprocess_training_file(ES_TRAIN_DATA_FILE)\n",
    "ES_test_words = _preprocess_test_file(ES_TEST_DATA_FILE)\n",
    "ES_unique_tags = get_unique(ES_tags)\n",
    "ES_unique_words = get_unique(ES_train_words)\n",
    "\n",
    "ES_emission_parameters = get_emission_using_MLE(ES_unique_tags, ES_unique_words, ES_tags, ES_train_words)\n",
    "ES_transition_pair_count = get_transition_pairs(ES_tags_with_start_stop)\n",
    "ES_transition_parameters = get_transition_using_MLE(ES_unique_tags, ES_tags_with_start_stop, ES_transition_pair_count)\n",
    "ES_unknown_words = get_unknown_words(ES_train_words, ES_test_words)\n",
    "\n",
    "ES_emission_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_probable_tag(emission):\n",
    "    highest_prob = {}\n",
    "    output = {}\n",
    "\n",
    "    for tag in emission:\n",
    "        for word in emission[tag]:\n",
    "            prob = emission[tag][word]\n",
    "            if word not in highest_prob:\n",
    "                highest_prob[word] = prob\n",
    "                output[word] = tag\n",
    "            else:\n",
    "                if prob > highest_prob[word]:\n",
    "                    highest_prob[word] = prob\n",
    "                    output[word] = tag\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_predicted_file_part1(predicted_file, test_file, most_probable_tag):\n",
    "    with open(predicted_file, \"w\", encoding=\"utf8\") as f:\n",
    "        for seq in test_file:\n",
    "            for word in seq:\n",
    "                if len(word) > 0:\n",
    "                    try:\n",
    "                        y = most_probable_tag[word]\n",
    "                    except:\n",
    "                        y = most_probable_tag[UNK_WORD]\n",
    "                    f.write(f\"{word} {y}\\n\")\n",
    "                else: # leave the newlines??\n",
    "                    f.write(\"\\n\")\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 output\n",
    "ES_most_probable_tag = get_most_probable_tag(ES_emission_parameters)\n",
    "write_to_predicted_file_part1(ES_PART1_PREDICTED_DATA_FILE, ES_test_words, ES_most_probable_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EvalScript.evalResult import get_observed, get_predicted,compare_observed_to_predicted\n",
    "\n",
    "def evaluateScores(actual_file, predicted_file):\n",
    "    with open(predicted_file, encoding=\"utf8\") as f:\n",
    "        predicted = f.read().splitlines()\n",
    "\n",
    "    with open(actual_file, encoding=\"utf8\") as f:\n",
    "        actual = f.read().splitlines()\n",
    "\n",
    "    compare_observed_to_predicted(get_observed(actual), get_predicted(predicted))\n",
    "\n",
    "evaluateScores(ES_ACTUAL_DATA_FILE, ES_PART1_PREDICTED_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# import sys\n",
    "\n",
    "# word_output_list = []  # list of tuple(word, predicted_tag) for writing to file\n",
    "# viterbi_values = {}  # key: (n, tag)  value: float\n",
    "\n",
    "# def NEW_init_viterbi_values(tags_unique):\n",
    "#     global viterbi_values\n",
    "#     viterbi_values = {(0, START_TAG): 0.0}\n",
    "\n",
    "#     for tag in tags_unique:\n",
    "#         viterbi_values[(0, tag)] = -sys.float_info.max\n",
    "\n",
    "\n",
    "# def NEW_generate_viterbi_values(n, current_tag, word_list, words_unique, tags_unique, emission_params, transmission_params):\n",
    "#     global viterbi_values\n",
    "#     current_max_viterbi_value = -sys.float_info.max\n",
    "#     value = -sys.float_info.max\n",
    "\n",
    "#     if n == 0:\n",
    "#         return\n",
    "\n",
    "#     # Recursive call to generate viterbi_values for (n-1, tag)\n",
    "#     for tag in tags_unique:\n",
    "#         if (n-1, tag) not in viterbi_values:\n",
    "#             NEW_generate_viterbi_values(n-1, tag, word_list, words_unique, tags_unique, emission_params, transmission_params)\n",
    "\n",
    "#     # Use viterbi values from n-1 to generate current viterbi value\n",
    "#     if n == 1:\n",
    "#         if word_list[n-1] not in words_unique:\n",
    "#             viterbi_values[(n, current_tag)] = viterbi_values[(n-1, START_TAG)] + math.log(emission_params[current_tag][UNK_WORD] * transmission_params[START_TAG][current_tag])\n",
    "#         else:\n",
    "#             try:\n",
    "#                 viterbi_values[(n, current_tag)] = viterbi_values[(n-1, START_TAG)] + math.log(emission_params[current_tag][word_list[n-1]] * transmission_params[START_TAG][current_tag])\n",
    "#             except ValueError:\n",
    "#                 viterbi_values[(n, current_tag)] = -sys.float_info.max\n",
    "#         return\n",
    "    \n",
    "#     # For n >= 2, search through tags\n",
    "#     for tag in tags_unique:\n",
    "#         try:\n",
    "#             if word_list[n-1] not in words_unique:\n",
    "#                 value = viterbi_values[(n-1, tag)] + math.log(emission_params[current_tag][UNK_WORD] * transmission_params[tag][current_tag])\n",
    "#             else:\n",
    "#                 try:\n",
    "#                     value = viterbi_values[(n-1, tag)] + math.log(emission_params[current_tag][word_list[n-1]] * transmission_params[tag][current_tag])\n",
    "#                 except KeyError:\n",
    "#                     continue\n",
    "#         except ValueError:\n",
    "#             continue\n",
    "\n",
    "#         current_max_viterbi_value = max(current_max_viterbi_value, value)\n",
    "\n",
    "#     viterbi_values[(n, current_tag)] = current_max_viterbi_value\n",
    "\n",
    "# def NEW_start_viterbi(word_list, words_unique, tags_unique, emission_params, transmission_params):\n",
    "#     global viterbi_values\n",
    "#     max_final_viterbi_value = -sys.float_info.max\n",
    "#     value = -sys.float_info.max\n",
    "\n",
    "#     n = len(word_list)\n",
    "\n",
    "#     # Recursive call to generate viterbi_values for (n, tag)\n",
    "#     for tag in tags_unique:\n",
    "#         NEW_generate_viterbi_values(n, tag, word_list, words_unique, tags_unique, emission_params, transmission_params)\n",
    "\n",
    "#     # Use viterbi values from n to generate viterbi value for (n+1, STOP)\n",
    "#     for tag in tags_unique:\n",
    "#         try:\n",
    "#             value = viterbi_values[(n, tag)] + math.log(transmission_params[tag][STOP_TAG])\n",
    "#         except ValueError:\n",
    "#             continue\n",
    "#         max_final_viterbi_value = max(max_final_viterbi_value, value)\n",
    "\n",
    "#     viterbi_values[(n+1, STOP_TAG)] = max_final_viterbi_value\n",
    "\n",
    "#     print(viterbi_values)\n",
    "\n",
    "\n",
    "# def generate_predictions_viterbi(word_list, tags_unique, emission_params, transmission_params):\n",
    "#     global viterbi_values\n",
    "\n",
    "#     n = len(word_list)\n",
    "\n",
    "#     generated_tag_list = [''] * n\n",
    "\n",
    "#     # Compute tag for n\n",
    "#     current_best_tag = ''\n",
    "#     current_best_tag_value = -sys.float_info.max\n",
    "\n",
    "#     for tag in tags_unique:\n",
    "#         try:\n",
    "#             value = viterbi_values[(n, tag)] + math.log(transmission_params[tag][STOP_TAG])\n",
    "#         except ValueError:\n",
    "#             continue\n",
    "#         if value > current_best_tag_value:\n",
    "#             current_best_tag = tag\n",
    "#             current_best_tag_value = value\n",
    "\n",
    "#     # if current_best_tag == '':\n",
    "#     #     current_best_tag = 'O'\n",
    "#     generated_tag_list[n-1] = current_best_tag #! unable to generate best tag sometimes, stuck at ''\n",
    "\n",
    "#     # Generate predictions starting from n-1 going down to 1\n",
    "#     for i in range(n-1, 0, -1):\n",
    "#         current_best_tag = ''\n",
    "#         current_best_tag_value = -sys.float_info.max\n",
    "\n",
    "#         for tag in tags_unique:\n",
    "#             try:\n",
    "#                 value = viterbi_values[(i, tag)] * transmission_params[tag][generated_tag_list[i]]\n",
    "\n",
    "#             except ValueError:\n",
    "#                 continue\n",
    "\n",
    "#             if value > current_best_tag_value:\n",
    "#                 current_best_tag = tag\n",
    "#                 current_best_tag_value = value\n",
    "\n",
    "#         generated_tag_list[i-1] = current_best_tag\n",
    "    \n",
    "#     return generated_tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_word = ['Con', 'lo', 'cual', 'en', 'el', 'comedor', 'tienes', 'que', 'levantar', 'mas', 'la', 'voz', 'para', 'oirte', 'y', 'se', 'forma', 'un', 'ambiente', 'que', 'no', 'lo', 'que', 'se', 'espera', 'de', 'una', 'estrella', 'michelin', '.']\n",
    "# NEW_init_viterbi_values(ES_unique_tags)\n",
    "# NEW_start_viterbi(test_word, ES_unique_words, ES_unique_tags, ES_emission_parameters, ES_transition_parameters)\n",
    "# print(viterbi_values)\n",
    "# ES_generated_tag_list = generate_predictions_viterbi(test_word, ES_unique_tags, ES_emission_parameters, ES_transition_parameters)\n",
    "# print(ES_generated_tag_list)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6bea1edf50713343d797f2d3f5675b7d568eeccbe5314af310a951584ccf1264"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('data-sci': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
