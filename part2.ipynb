{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_TRAIN_DATA_FILE = \"./ES/train\"\n",
    "ES_TEST_DATA_FILE = \"./ES/dev.in\"\n",
    "ES_ACTUAL_DATA_FILE = \"./ES/dev.out\"\n",
    "ES_PART1_PREDICTED_DATA_FILE = \"./ES/dev.p1.out\"\n",
    "ES_PART2_PREDICTED_DATA_FILE = \"./ES/dev.p2.out\"\n",
    "ES_PART3_PREDICTED_DATA_FILE = \"./ES/dev.p3.out\"\n",
    "ES_PART4_PREDICTED_DATA_FILE = \"./ES/dev.p4.out\"\n",
    "\n",
    "RU_TRAIN_DATA_FILE = \"./RU/train\"\n",
    "RU_TEST_DATA_FILE = \"./RU/dev.in\"\n",
    "RU_ACTUAL_DATA_FILE = \"./RU/dev.out\"\n",
    "RU_PART1_PREDICTED_DATA_FILE = \"./RU/dev.p1.out\"\n",
    "RU_PART2_PREDICTED_DATA_FILE = \"./RU/dev.p2.out\"\n",
    "RU_PART3_PREDICTED_DATA_FILE = \"./RU/dev.p3.out\"\n",
    "RU_PART4_PREDICTED_DATA_FILE = \"./RU/dev.p4.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def _preprocess_training_file(training_file):\n",
    "    tags = []\n",
    "    tags_with_start_stop = []\n",
    "    words = []\n",
    "\n",
    "    with open(training_file,\"r\",encoding=\"utf8\") as f:\n",
    "        document = f.read().rstrip()\n",
    "        lines = document.split(\"\\n\\n\")\n",
    "\n",
    "        for line in lines:\n",
    "            tags_list = []\n",
    "            tags_with_start_stop_list = []\n",
    "            words_list = []\n",
    "\n",
    "            for word_tag in line.split(\"\\n\"):\n",
    "                i = word_tag.split(\" \")\n",
    "\n",
    "                if len(i) > 2:\n",
    "                    i = [\" \".join(i[0:len(i)-1]), i[len(i)-1]]\n",
    "\n",
    "                word, tag = i[0], i[1]\n",
    "                words_list.append(word)\n",
    "                tags_list.append(tag)\n",
    "\n",
    "            tags.append(tags_list)\n",
    "            tags_with_start_stop_list = [\"START\"] + tags_list + [\"STOP\"]\n",
    "            tags_with_start_stop.append(tags_with_start_stop_list)\n",
    "            words.append(words_list)\n",
    "    \n",
    "    return tags, tags_with_start_stop, words\n",
    "\n",
    "def _preprocess_test_file(testing_file):\n",
    "    test_words = []\n",
    "\n",
    "    with open(testing_file,\"r\",encoding=\"utf8\") as f:\n",
    "        document = f.read().rstrip()\n",
    "        lines = document.split(\"\\n\\n\")\n",
    "\n",
    "        for line in lines:\n",
    "            word_list = []\n",
    "            for word in line.split(\"\\n\"):\n",
    "                word_list.append(word)\n",
    "            test_words.append(word_list)\n",
    "\n",
    "    return test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique(x):\n",
    "    return list(set(list(itertools.chain.from_iterable(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_pairs(tags):\n",
    "    transition_pair_count = {}\n",
    "\n",
    "    for tag in tags:\n",
    "        for tag1, tag2 in zip(tag[:-1], tag[1:]):\n",
    "            transition_pair = (tag1, tag2)\n",
    "            if transition_pair in transition_pair_count:\n",
    "                transition_pair_count[transition_pair] += 1\n",
    "            else:\n",
    "                transition_pair_count[transition_pair] = 1\n",
    "\n",
    "    return transition_pair_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_y(tag, tags):\n",
    "    tags_flattened = list(itertools.chain.from_iterable(tags))\n",
    "    return tags_flattened.count(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emission_using_MLE(unique_tags, unique_words, tags, words, k=1):\n",
    "    emission = {}\n",
    "    unique_tags.extend([\"START\", \"STOP\"])\n",
    "    for tag in unique_tags:\n",
    "        emission_word = {}\n",
    "        for word in unique_words:\n",
    "            emission_word[word] = 0.0\n",
    "        emission_word[\"#UNK#\"] = 0.0\n",
    "        emission[tag] = emission_word\n",
    "    # fill up emission dictionary\n",
    "    for tag_sentence, word_sentence in zip(tags, words):\n",
    "        for tag, word in zip(tag_sentence, word_sentence):\n",
    "            emission[tag][word] += 1\n",
    "            \n",
    "    for tag, emission_word in emission.items():\n",
    "        tag_count = count_y(tag, tags) + k\n",
    "        for word, count_y_x in emission_word.items():\n",
    "            emission[tag][word] = count_y_x / (tag_count)\n",
    "\n",
    "        emission[tag][\"#UNK#\"] = k / (tag_count)\n",
    "    return emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_using_MLE(unique_tags, tags_with_start_stop, transition_pair_count):\n",
    "    unique_tags = [\"START\"] + unique_tags + [\"STOP\"]\n",
    "    transition = {}\n",
    "    for u in unique_tags[:-1]: # omit STOP\n",
    "        transition_tag = {}\n",
    "        for v in unique_tags[1:]: # omit START\n",
    "            transition_tag[v] = 0.0\n",
    "        transition[u] = transition_tag\n",
    "\n",
    "    # fill up transition parameters\n",
    "    for u, v in transition_pair_count:\n",
    "        transition[u][v] += transition_pair_count[(u, v)]\n",
    "    \n",
    "    # divide transition_count by count_yi, to get probability\n",
    "    for u, transition_tag in transition.items():\n",
    "        count_yi = count_y(u, tags_with_start_stop)\n",
    "        # words in training set\n",
    "        for v, transition_count in transition_tag.items():\n",
    "            if count_yi == 0:\n",
    "                transition[u][v] = 0.0\n",
    "            else:\n",
    "                transition[u][v] = transition_count / count_yi\n",
    "\n",
    "    return transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unknown_words(train, test):\n",
    "    test_words = get_unique(test)\n",
    "    train_words = get_unique(train)\n",
    "    return set(test_words) - set(train_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_tags, ES_tags_with_start_stop, ES_train_words = _preprocess_training_file(ES_TRAIN_DATA_FILE)\n",
    "ES_test_words = _preprocess_test_file(ES_TEST_DATA_FILE)\n",
    "ES_unique_tags = get_unique(ES_tags)\n",
    "ES_unique_words = get_unique(ES_train_words)\n",
    "ES_emission_parameters = get_emission_using_MLE(ES_unique_tags, ES_unique_words, ES_tags, ES_train_words)\n",
    "\n",
    "ES_transition_pair_count = get_transition_pairs(ES_tags_with_start_stop)\n",
    "ES_transition_parameters = get_transition_using_MLE(ES_unique_tags, ES_tags_with_start_stop, ES_transition_pair_count)\n",
    "\n",
    "ES_unknown_words = get_unknown_words(ES_train_words, ES_test_words)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6bea1edf50713343d797f2d3f5675b7d568eeccbe5314af310a951584ccf1264"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('data-sci': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
