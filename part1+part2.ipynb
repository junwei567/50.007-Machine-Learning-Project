{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "x_f4Mywc7Wav"
      },
      "outputs": [],
      "source": [
        "ES_TRAIN_DATA_FILE = \"./ES/train\"\n",
        "ES_TEST_DATA_FILE = \"./ES/dev.in\"\n",
        "ES_ACTUAL_DATA_FILE = \"./ES/dev.out\"\n",
        "ES_PART1_PREDICTED_DATA_FILE = \"./ES/dev.p1.out\"\n",
        "ES_PART2_PREDICTED_DATA_FILE = \"./ES/dev.p2.out\"\n",
        "ES_PART3_PREDICTED_DATA_FILE = \"./ES/dev.p3.out\"\n",
        "ES_PART4_PREDICTED_DATA_FILE = \"./ES/dev.p4.out\"\n",
        "\n",
        "RU_TRAIN_DATA_FILE = \"./RU/train\"\n",
        "RU_TEST_DATA_FILE = \"./RU/dev.in\"\n",
        "RU_ACTUAL_DATA_FILE = \"./RU/dev.out\"\n",
        "RU_PART1_PREDICTED_DATA_FILE = \"./RU/dev.p1.out\"\n",
        "RU_PART2_PREDICTED_DATA_FILE = \"./RU/dev.p2.out\"\n",
        "RU_PART3_PREDICTED_DATA_FILE = \"./RU/dev.p3.out\"\n",
        "RU_PART4_PREDICTED_DATA_FILE = \"./RU/dev.p4.out\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1 - Emission Parameters, UNK Token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "lQg0WHpM7Wa0"
      },
      "outputs": [],
      "source": [
        "def _preprocess_training_file(file_path):\n",
        "    with open(file_path, encoding=\"utf8\") as f:\n",
        "        data = f.read().splitlines()\n",
        "        data[:] = [x for x in data if x]\n",
        "\n",
        "    output = []\n",
        "    for i in data:\n",
        "        i = i.split(\" \")\n",
        "        if(len(i) > 2):\n",
        "            i = [\" \".join(i[0:len(i)-1]), i[len(i)-1]]\n",
        "            output.append(i)\n",
        "        else:\n",
        "            output.append(i)\n",
        "        \n",
        "    return output\n",
        "\n",
        "def _preprocess_testing_file(path):\n",
        "    with open(path, encoding=\"utf8\") as f:\n",
        "        data = f.read().splitlines()\n",
        "\n",
        "    output = []\n",
        "    for word in data:\n",
        "        # if word: # leave the newlines or not?\n",
        "        output.append(word)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "EoDYOryl7Wa1"
      },
      "outputs": [],
      "source": [
        "def get_emission_using_MLE(training, k = 1):\n",
        "    tags = {}\n",
        "    tags_to_word = {}\n",
        "    emission = {}\n",
        "    for data in training:\n",
        "        word, tag = data[0], data[1]\n",
        "        if tag in tags:\n",
        "            tags[tag] += 1\n",
        "        else:\n",
        "            tags[tag] = 1\n",
        "        \n",
        "        tag_to_word = tuple((tag, word))\n",
        "        if tag_to_word in tags_to_word:\n",
        "            tags_to_word[tag_to_word] += 1\n",
        "        else:\n",
        "            tags_to_word[tag_to_word] = 1\n",
        "\n",
        "    for key in tags_to_word.keys():\n",
        "        emission[key] = tags_to_word[key] / (tags[key[0]] + k)\n",
        "    for key in tags.keys():\n",
        "        transition = tuple((key, \"#UNK#\"))\n",
        "        emission[transition] = k / (tags[key] + k)\n",
        "    # print(emission)\n",
        "    return emission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "p4FTr7EO7Wa2"
      },
      "outputs": [],
      "source": [
        "def get_most_probable_tag(emission):\n",
        "    highest_prob = {}\n",
        "    output = {}\n",
        "    for key, prob in emission.items():\n",
        "        tag, word = key[0], key[1]\n",
        "        if word not in highest_prob:\n",
        "            highest_prob[word] = prob\n",
        "            output[word] = tag\n",
        "        else:\n",
        "            if prob > highest_prob[word]:\n",
        "                highest_prob[word] = prob\n",
        "                output[word] = tag\n",
        "    # print(output)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "6UGSa6o8oSq-"
      },
      "outputs": [],
      "source": [
        "def get_unique_elements(lst):\n",
        "    return list(set(list(itertools.chain.from_iterable(lst))))\n",
        "\n",
        "def get_unique_tags(tags):\n",
        "    tags_unique = get_unique_elements(tags)\n",
        "    print(f'tags_unique: {tags_unique}')\n",
        "    tags_unique.sort()\n",
        "    print(f'tags_unique_sorted: {tags_unique}')\n",
        "    tags_unique_with_start_stop = [\"START\"] + tags_unique + [\"STOP\"]\n",
        "    print(f'tags_unique_with_start_stop: {tags_unique_with_start_stop}\\n')\n",
        "\n",
        "    return tags_unique, tags_unique_with_start_stop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "d4wyxNfF7Wa2"
      },
      "outputs": [],
      "source": [
        "def write_to_predicted_file_part1(predicted_file, test_file, most_probable_tag):\n",
        "    with open(predicted_file, \"w\", encoding=\"utf8\") as f:\n",
        "        for word in test_file:\n",
        "            if len(word) > 0:\n",
        "                try:\n",
        "                    y = most_probable_tag[word]\n",
        "                except:\n",
        "                    y = most_probable_tag[\"#UNK#\"]\n",
        "                f.write(f\"{word} {y}\\n\")\n",
        "            else: # leave the newlines??\n",
        "                f.write(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "jXeN5YOL7Wa3"
      },
      "outputs": [],
      "source": [
        "# Part 1 code for ES Dataset\n",
        "ES_train_data = _preprocess_training_file(ES_TRAIN_DATA_FILE)\n",
        "ES_test_data = _preprocess_testing_file(ES_TEST_DATA_FILE)\n",
        "ES_unique_words = get_unique_elements(ES_train_data)\n",
        "\n",
        "ES_emission_parameters = get_emission_using_MLE(ES_train_data)\n",
        "ES_most_probable_tag = get_most_probable_tag(ES_emission_parameters)\n",
        "write_to_predicted_file_part1(ES_PART1_PREDICTED_DATA_FILE, ES_test_data, ES_most_probable_tag)\n",
        "\n",
        "# Part 1 code for RU Dataset\n",
        "RU_train_data = _preprocess_training_file(RU_TRAIN_DATA_FILE)\n",
        "RU_test_data = _preprocess_testing_file(RU_TEST_DATA_FILE)\n",
        "RU_unique_words = get_unique_elements(RU_train_data)\n",
        "\n",
        "RU_emission_parameters = get_emission_using_MLE(RU_train_data)\n",
        "RU_most_probable_tag = get_most_probable_tag(RU_emission_parameters)\n",
        "write_to_predicted_file_part1(RU_PART1_PREDICTED_DATA_FILE, RU_test_data, RU_most_probable_tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "qKJsLOOC7Wa4",
        "outputId": "4d84d753-2a74-4f45-a3bc-09891a7451a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "#Entity in gold data: 255\n",
            "#Entity in prediction: 1733\n",
            "\n",
            "#Correct Entity : 205\n",
            "Entity  precision: 0.1183\n",
            "Entity  recall: 0.8039\n",
            "Entity  F: 0.2062\n",
            "\n",
            "#Correct Sentiment : 113\n",
            "Sentiment  precision: 0.0652\n",
            "Sentiment  recall: 0.4431\n",
            "Sentiment  F: 0.1137\n"
          ]
        }
      ],
      "source": [
        "from EvalScript.evalResult import get_observed, get_predicted,compare_observed_to_predicted\n",
        "\n",
        "def evaluateScores(actual_file, predicted_file):\n",
        "    with open(predicted_file, encoding=\"utf8\") as f:\n",
        "        predicted = f.read().splitlines()\n",
        "\n",
        "    with open(actual_file, encoding=\"utf8\") as f:\n",
        "        actual = f.read().splitlines()\n",
        "   \n",
        "    compare_observed_to_predicted(get_observed(actual), get_predicted(predicted))\n",
        "\n",
        "evaluateScores(ES_ACTUAL_DATA_FILE, ES_PART1_PREDICTED_DATA_FILE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2i - Transition Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "Ts1g5HEZoQTh"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "def _preprocess_training_file(training_file):\n",
        "    tags = []\n",
        "    tags_with_start_stop = []\n",
        "    words = []\n",
        "\n",
        "    with open(training_file,\"r\",encoding=\"utf8\") as f:\n",
        "        document = f.read().rstrip()\n",
        "        lines = document.split(\"\\n\\n\")\n",
        "\n",
        "        for line in lines:\n",
        "            tags_list = []\n",
        "            tags_with_start_stop_list = []\n",
        "            words_list = []\n",
        "\n",
        "            for word_tag in line.split(\"\\n\"):\n",
        "                i = word_tag.split(\" \")\n",
        "\n",
        "                if len(i) > 2:\n",
        "                    i = [\" \".join(i[0:len(i)-1]), i[len(i)-1]]\n",
        "\n",
        "                word, tag = i[0], i[1]\n",
        "                words_list.append(word)\n",
        "                tags_list.append(tag)\n",
        "\n",
        "            tags.append(tags_list)\n",
        "            tags_with_start_stop_list = [\"START\"] + tags_list + [\"STOP\"]\n",
        "            tags_with_start_stop.append(tags_with_start_stop_list)\n",
        "            words.append(words_list)\n",
        "    \n",
        "    print(f'tags: {tags[0:3]}')\n",
        "    print(f'tags_with_start_stop: {tags_with_start_stop[0:3]}')\n",
        "    print(f'words: {words[0:3]}\\n')\n",
        "    return tags, tags_with_start_stop, words\n",
        "\n",
        "def _preprocess_test_file(testing_file):\n",
        "    test_words = []\n",
        "\n",
        "    with open(testing_file,\"r\",encoding=\"utf8\") as f:\n",
        "        document = f.read().rstrip()\n",
        "        lines = document.split(\"\\n\\n\")\n",
        "\n",
        "        for line in lines:\n",
        "            word_list = []\n",
        "            for word in line.split(\"\\n\"):\n",
        "                word_list.append(word)\n",
        "            test_words.append(word_list)\n",
        "\n",
        "    return test_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "jiPUAxu8oWaC"
      },
      "outputs": [],
      "source": [
        "def get_transition_pairs(tags):\n",
        "    transition_pair_count = {}\n",
        "\n",
        "    for tag in tags:\n",
        "        #yi-1 and yi tuples\n",
        "        for tag1, tag2 in zip(tag[:-1], tag[1:]):\n",
        "            transition_pair = (tag1, tag2)\n",
        "            if transition_pair in transition_pair_count:\n",
        "                transition_pair_count[transition_pair] += 1\n",
        "            else:\n",
        "                transition_pair_count[transition_pair] = 1\n",
        "\n",
        "    print(f'transition_pair_count: {transition_pair_count}\\n')\n",
        "    return transition_pair_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "oFuy2BmcoZTC"
      },
      "outputs": [],
      "source": [
        "def count_y(tag, tags):\n",
        "    tags_flattened = list(itertools.chain.from_iterable(tags))\n",
        "    return tags_flattened.count(tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "XtJUBQ3uofA7"
      },
      "outputs": [],
      "source": [
        "def get_transition_using_MLE(tags_unique_with_start_stop, transition_pair_count,\n",
        "                          tags_with_start_stop):\n",
        "\n",
        "    transition = {}\n",
        "    #create matrix dimensions\n",
        "    for tag1 in tags_unique_with_start_stop[:-1]:\n",
        "        transition_row = {}\n",
        "        for tag2 in tags_unique_with_start_stop[1:]:\n",
        "            transition_row[tag2] = 0.0\n",
        "        transition[tag1] = transition_row\n",
        "\n",
        "    print(f'transition_params_init: {transition}\\n')\n",
        "    # populate transition parameters with counts\n",
        "    for tag1, tag2 in transition_pair_count:\n",
        "        transition[tag1][tag2] += transition_pair_count[(tag1, tag2)]\n",
        "    \n",
        "    print(f'transition_params_with_count: {transition}\\n')\n",
        "\n",
        "    # divide transition_count by count_yi, to get probability\n",
        "    for tag1, transition_row in transition.items():\n",
        "        count_yi = count_y(tag1, tags_with_start_stop)\n",
        "\n",
        "        # words in training set\n",
        "        for tag2, transition_count in transition_row.items():\n",
        "            if count_yi == 0:\n",
        "                transition[tag1][tag2] = 0.0\n",
        "            else:\n",
        "                transition[tag1][tag2] = transition_count / count_yi\n",
        "\n",
        "    print(f'transition_params_final: {transition}\\n')\n",
        "\n",
        "    return transition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tags: [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-positive', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'B-negative', 'O']]\n",
            "tags_with_start_stop: [['START', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-positive', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'STOP'], ['START', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'STOP'], ['START', 'O', 'O', 'B-negative', 'O', 'STOP']]\n",
            "words: [['disfrutemos', 'de', 'una', 'buenísima', 'calidad', 'en', 'el', 'producto', 'y', 'una', 'inmejorable', 'relación', 'calidad', 'precio', '.'], ['Hoy', 'he', 'ido', 'a', 'comer', 'con', 'mia', 'padres', 'y', 'he', 'salido', 'muy', 'defraudado', '.'], ['A', 'mejorar', 'baños', '.']]\n",
            "\n",
            "tags_unique: ['I-positive', 'B-neutral', 'I-neutral', 'O', 'I-negative', 'B-positive', 'B-negative']\n",
            "tags_unique_sorted: ['B-negative', 'B-neutral', 'B-positive', 'I-negative', 'I-neutral', 'I-positive', 'O']\n",
            "tags_unique_with_start_stop: ['START', 'B-negative', 'B-neutral', 'B-positive', 'I-negative', 'I-neutral', 'I-positive', 'O', 'STOP']\n",
            "\n",
            "transition_pair_count: {('START', 'O'): 1918, ('O', 'O'): 27939, ('O', 'B-positive'): 1162, ('B-positive', 'O'): 1100, ('O', 'STOP'): 2050, ('O', 'B-negative'): 402, ('B-negative', 'O'): 347, ('START', 'B-negative'): 27, ('B-negative', 'I-negative'): 78, ('I-negative', 'I-negative'): 151, ('I-negative', 'O'): 78, ('START', 'B-positive'): 110, ('B-positive', 'I-positive'): 162, ('I-positive', 'O'): 160, ('I-positive', 'I-positive'): 238, ('O', 'B-neutral'): 74, ('B-neutral', 'O'): 69, ('START', 'B-neutral'): 10, ('B-positive', 'B-positive'): 2, ('B-negative', 'STOP'): 4, ('B-positive', 'STOP'): 9, ('B-neutral', 'I-neutral'): 16, ('I-neutral', 'O'): 16, ('I-positive', 'STOP'): 2, ('B-positive', 'B-neutral'): 1, ('I-neutral', 'I-neutral'): 28}\n",
            "\n",
            "transition_params_init: {'START': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 0.0, 'STOP': 0.0}, 'B-negative': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 0.0, 'STOP': 0.0}, 'B-neutral': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 0.0, 'STOP': 0.0}, 'B-positive': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 0.0, 'STOP': 0.0}, 'I-negative': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 0.0, 'STOP': 0.0}, 'I-neutral': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 0.0, 'STOP': 0.0}, 'I-positive': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 0.0, 'STOP': 0.0}, 'O': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 0.0, 'STOP': 0.0}}\n",
            "\n",
            "transition_params_with_count: {'START': {'B-negative': 27.0, 'B-neutral': 10.0, 'B-positive': 110.0, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 1918.0, 'STOP': 0.0}, 'B-negative': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 78.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 347.0, 'STOP': 4.0}, 'B-neutral': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.0, 'I-neutral': 16.0, 'I-positive': 0.0, 'O': 69.0, 'STOP': 0.0}, 'B-positive': {'B-negative': 0.0, 'B-neutral': 1.0, 'B-positive': 2.0, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 162.0, 'O': 1100.0, 'STOP': 9.0}, 'I-negative': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 151.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 78.0, 'STOP': 0.0}, 'I-neutral': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.0, 'I-neutral': 28.0, 'I-positive': 0.0, 'O': 16.0, 'STOP': 0.0}, 'I-positive': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 238.0, 'O': 160.0, 'STOP': 2.0}, 'O': {'B-negative': 402.0, 'B-neutral': 74.0, 'B-positive': 1162.0, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 27939.0, 'STOP': 2050.0}}\n",
            "\n",
            "transition_params_final: {'START': {'B-negative': 0.013075060532687652, 'B-neutral': 0.004842615012106538, 'B-positive': 0.053268765133171914, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 0.9288135593220339, 'STOP': 0.0}, 'B-negative': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.18181818181818182, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 0.8088578088578089, 'STOP': 0.009324009324009324}, 'B-neutral': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.0, 'I-neutral': 0.18823529411764706, 'I-positive': 0.0, 'O': 0.8117647058823529, 'STOP': 0.0}, 'B-positive': {'B-negative': 0.0, 'B-neutral': 0.0007849293563579278, 'B-positive': 0.0015698587127158557, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.1271585557299843, 'O': 0.8634222919937206, 'STOP': 0.00706436420722135}, 'I-negative': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.6593886462882096, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 0.3406113537117904, 'STOP': 0.0}, 'I-neutral': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.0, 'I-neutral': 0.6363636363636364, 'I-positive': 0.0, 'O': 0.36363636363636365, 'STOP': 0.0}, 'I-positive': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.595, 'O': 0.4, 'STOP': 0.005}, 'O': {'B-negative': 0.012710658614474974, 'B-neutral': 0.0023397729787839505, 'B-positive': 0.03674075947766149, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 0.8833907737060107, 'STOP': 0.06481803522306889}}\n",
            "\n",
            "tags: [['O', 'O', 'O', 'O', 'B-positive', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'B-positive', 'O', 'O', 'B-positive', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
            "tags_with_start_stop: [['START', 'O', 'O', 'O', 'O', 'B-positive', 'O', 'O', 'O', 'O', 'O', 'STOP'], ['START', 'O', 'O', 'O', 'O', 'B-positive', 'O', 'O', 'B-positive', 'O', 'O', 'O', 'STOP'], ['START', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'STOP']]\n",
            "words: [['А', 'жаль', '...', 'новое', 'место', 'в', 'центре', 'города', '!', '!'], ['В', '-', 'третьих', ',', 'обслуживание', 'и', 'отношение', 'персонала', 'очень', 'радушное', '.'], ['Н', 'Начну', 'из', 'далека', ')', ')', 'года', '1,5-', '2', 'назад', 'заходила', 'в', '\"', 'дали', '\"', 'и', 'осталась', 'с', 'неоднозначными', 'впечатлениями', '(', 'то', 'ли', 'понравилось', ',', 'то', 'ли', 'нет', ')', '.']]\n",
            "\n",
            "tags_unique: ['I-positive', 'B-neutral', 'I-neutral', 'O', 'I-negative', 'B-positive', 'B-negative']\n",
            "tags_unique_sorted: ['B-negative', 'B-neutral', 'B-positive', 'I-negative', 'I-neutral', 'I-positive', 'O']\n",
            "tags_unique_with_start_stop: ['START', 'B-negative', 'B-neutral', 'B-positive', 'I-negative', 'I-neutral', 'I-positive', 'O', 'STOP']\n",
            "\n",
            "transition_pair_count: {('START', 'O'): 2889, ('O', 'O'): 38975, ('O', 'B-positive'): 1736, ('B-positive', 'O'): 1717, ('O', 'STOP'): 3401, ('START', 'B-positive'): 379, ('O', 'B-neutral'): 144, ('B-neutral', 'O'): 190, ('B-positive', 'I-positive'): 398, ('I-positive', 'O'): 396, ('I-positive', 'I-positive'): 293, ('O', 'B-negative'): 384, ('B-negative', 'O'): 367, ('START', 'B-negative'): 58, ('START', 'B-neutral'): 78, ('B-negative', 'I-negative'): 75, ('I-negative', 'I-negative'): 49, ('I-negative', 'O'): 74, ('B-neutral', 'I-neutral'): 32, ('I-neutral', 'I-neutral'): 38, ('I-neutral', 'O'): 32, ('B-positive', 'B-positive'): 2, ('I-negative', 'STOP'): 1, ('I-positive', 'B-positive'): 1, ('I-positive', 'STOP'): 1, ('B-positive', 'STOP'): 1}\n",
            "\n",
            "transition_params_init: {'START': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 0.0, 'STOP': 0.0}, 'B-negative': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 0.0, 'STOP': 0.0}, 'B-neutral': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 0.0, 'STOP': 0.0}, 'B-positive': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 0.0, 'STOP': 0.0}, 'I-negative': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 0.0, 'STOP': 0.0}, 'I-neutral': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 0.0, 'STOP': 0.0}, 'I-positive': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 0.0, 'STOP': 0.0}, 'O': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 0.0, 'STOP': 0.0}}\n",
            "\n",
            "transition_params_with_count: {'START': {'B-negative': 58.0, 'B-neutral': 78.0, 'B-positive': 379.0, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 2889.0, 'STOP': 0.0}, 'B-negative': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 75.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 367.0, 'STOP': 0.0}, 'B-neutral': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.0, 'I-neutral': 32.0, 'I-positive': 0.0, 'O': 190.0, 'STOP': 0.0}, 'B-positive': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 2.0, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 398.0, 'O': 1717.0, 'STOP': 1.0}, 'I-negative': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 49.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 74.0, 'STOP': 1.0}, 'I-neutral': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.0, 'I-neutral': 38.0, 'I-positive': 0.0, 'O': 32.0, 'STOP': 0.0}, 'I-positive': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 1.0, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 293.0, 'O': 396.0, 'STOP': 1.0}, 'O': {'B-negative': 384.0, 'B-neutral': 144.0, 'B-positive': 1736.0, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 38975.0, 'STOP': 3401.0}}\n",
            "\n",
            "transition_params_final: {'START': {'B-negative': 0.017038777908343124, 'B-neutral': 0.022914218566392478, 'B-positive': 0.11133960047003526, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 0.8487074030552292, 'STOP': 0.0}, 'B-negative': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.16968325791855204, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 0.830316742081448, 'STOP': 0.0}, 'B-neutral': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.0, 'I-neutral': 0.14414414414414414, 'I-positive': 0.0, 'O': 0.8558558558558559, 'STOP': 0.0}, 'B-positive': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0009442870632672333, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.18791312559017942, 'O': 0.8106704438149197, 'STOP': 0.00047214353163361664}, 'I-negative': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.3951612903225806, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 0.5967741935483871, 'STOP': 0.008064516129032258}, 'I-neutral': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.0, 'I-negative': 0.0, 'I-neutral': 0.5428571428571428, 'I-positive': 0.0, 'O': 0.45714285714285713, 'STOP': 0.0}, 'I-positive': {'B-negative': 0.0, 'B-neutral': 0.0, 'B-positive': 0.001447178002894356, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.4240231548480463, 'O': 0.573082489146165, 'STOP': 0.001447178002894356}, 'O': {'B-negative': 0.008602150537634409, 'B-neutral': 0.0032258064516129032, 'B-positive': 0.03888888888888889, 'I-negative': 0.0, 'I-neutral': 0.0, 'I-positive': 0.0, 'O': 0.8730958781362007, 'STOP': 0.07618727598566308}}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ES_tags, ES_tags_with_start_stop, ES_train_words = _preprocess_training_file(ES_TRAIN_DATA_FILE)\n",
        "ES_test_words = _preprocess_test_file(ES_TEST_DATA_FILE)\n",
        "ES_tags_unique, ES_tags_unique_with_start_stop = get_unique_tags(ES_tags)\n",
        "ES_transition_pair_count = get_transition_pairs(ES_tags_with_start_stop)\n",
        "ES_transition_parameters = get_transition_using_MLE(ES_tags_unique_with_start_stop, ES_transition_pair_count, ES_tags_with_start_stop)\n",
        "\n",
        "RU_tags, RU_tags_with_start_stop, RU_train_words = _preprocess_training_file(RU_TRAIN_DATA_FILE)\n",
        "RU_test_words = _preprocess_test_file(RU_TEST_DATA_FILE)\n",
        "RU_tags_unique, RU_tags_unique_with_start_stop = get_unique_tags(RU_tags)\n",
        "RU_transition_pair_count = get_transition_pairs(RU_tags_with_start_stop)\n",
        "RU_transition_parameters = get_transition_using_MLE(RU_tags_unique_with_start_stop, RU_transition_pair_count, RU_tags_with_start_stop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2ii - Viterbi Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Song Gee to write algorithm for Viterbi\n",
        "import math\n",
        "import sys\n",
        "\n",
        "word_output_list = []  # list of tuple(word, predicted_tag) for writing to file\n",
        "viterbi_values = {}  # key: (n, tag)  value: float \n",
        "\n",
        "def generate_viterbi_values(n, current_tag, word_list, words_unique, tags_unique, emission_params, transmission_params):\n",
        "    global viterbi_values\n",
        "    current_max_viterbi_value = -sys.float_info.max  # Smallest negative float\n",
        "\n",
        "    if n == 0:\n",
        "        return\n",
        "\n",
        "    if n == 1:\n",
        "        try:\n",
        "            if word_list[n-1] in words_unique:\n",
        "                try:\n",
        "                    current_max_viterbi_value = math.log(emission_params[(current_tag, word_list[n-1])] * transmission_params[\"START\"][current_tag])\n",
        "                except KeyError:\n",
        "                    current_max_viterbi_value = -sys.float_info.max\n",
        "            else:\n",
        "                current_max_viterbi_value = math.log(emission_params[(current_tag, \"#UNK#\")] * transmission_params[\"START\"][current_tag])\n",
        "        except ValueError:\n",
        "            current_max_viterbi_value = -sys.float_info.max\n",
        "        \n",
        "        viterbi_values[(n, current_tag)] = current_max_viterbi_value\n",
        "        return\n",
        "        \n",
        "\n",
        "    # Recursive call to generate viterbi_values for (n-1, tag)\n",
        "    for tag in tags_unique:\n",
        "        if (n-1, tag) not in viterbi_values:\n",
        "            generate_viterbi_values(n-1, tag, word_list, words_unique, tags_unique, emission_params, transmission_params)\n",
        "\n",
        "    # Use viterbi values from n-1 to generate current viterbi value\n",
        "    for tag in tags_unique:\n",
        "        # Here, we use a try-except block because our emission parameters only contain emissions which appeared in our datasets\n",
        "        # Thus, any unobserved emission will throw a KeyError, however it's value should be -inf, so we just catch the Error and proceed to the next tag\n",
        "        # If transmission_params gives 0, math.log will throw a valueError, thus we catch it and skip the current tag since 0 means we should never consider it\n",
        "        try:\n",
        "            if word_list[n-1] in words_unique:\n",
        "                try:\n",
        "                    value = viterbi_values[(n-1, tag)] + math.log(emission_params[(current_tag, word_list[n-1])] * transmission_params[tag][current_tag])\n",
        "                except KeyError:\n",
        "                    continue\n",
        "            else:\n",
        "                value = viterbi_values[(n-1, tag)] + math.log(emission_params[(current_tag, \"#UNK#\")] * transmission_params[tag][current_tag])\n",
        "        except ValueError:\n",
        "            continue\n",
        "        \n",
        "        # print(f'n: {n}, current_tag: {current_tag}, tag: {tag}, value: {value}')\n",
        "        current_max_viterbi_value = max(current_max_viterbi_value, value)\n",
        "    \n",
        "    viterbi_values[(n, current_tag)] = current_max_viterbi_value\n",
        "\n",
        "\n",
        "# function to kickstart viterbi recursive chain, and add (n+1, STOP) to veterbi_values\n",
        "def start_viterbi(word_list, words_unique, tags_unique, emission_params, transmission_params):\n",
        "    global viterbi_values\n",
        "    max_final_viterbi_value = -sys.float_info.max\n",
        "\n",
        "    n = len(word_list)\n",
        "\n",
        "    # Recursive call to generate viterbi_values for (n, tag)\n",
        "    for tag in tags_unique:\n",
        "        generate_viterbi_values(n, tag, word_list, words_unique, tags_unique, emission_params, transmission_params)\n",
        "\n",
        "    # Use viterbi values from n to generate viterbi value for (n+1, STOP)\n",
        "    for tag in tags_unique:\n",
        "        try:\n",
        "            value = viterbi_values[(n, tag)] + math.log(transmission_params[tag]['STOP'])\n",
        "        except ValueError:\n",
        "            continue\n",
        "        max_final_viterbi_value = max(max_final_viterbi_value, value)\n",
        "\n",
        "    viterbi_values[(n+1, 'STOP')] = max_final_viterbi_value\n",
        "\n",
        "\n",
        "def generate_predictions_viterbi(word_list, words_unique, tags_unique, emission_params, transmission_params):\n",
        "    global viterbi_values\n",
        "\n",
        "    n = len(word_list)\n",
        "\n",
        "    generated_tag_list = ['' for i in range(n)]\n",
        "\n",
        "    # Compute tag for n\n",
        "    current_best_tag = 'O'\n",
        "    current_best_tag_value = -sys.float_info.max\n",
        "\n",
        "    for tag in tags_unique:\n",
        "        try:\n",
        "            value = viterbi_values[(n, tag)] + math.log(transmission_params[tag]['STOP'])\n",
        "        except ValueError:\n",
        "            continue\n",
        "        if value > current_best_tag_value:\n",
        "            current_best_tag = tag\n",
        "            current_best_tag_value = value\n",
        "\n",
        "    generated_tag_list[n-1] = current_best_tag\n",
        "\n",
        "    # Generate predictions starting from n-1 going down to 1\n",
        "    for i in range(n-1, 0, -1):\n",
        "        current_best_tag = 'O'\n",
        "        current_best_tag_value = -sys.float_info.max\n",
        "\n",
        "        for tag in tags_unique:\n",
        "            try:\n",
        "                value = viterbi_values[(i, tag)] + math.log(transmission_params[tag][generated_tag_list[i]])\n",
        "            except ValueError:\n",
        "                continue\n",
        "            if value > current_best_tag_value:\n",
        "                current_best_tag = tag\n",
        "                current_best_tag_value = value\n",
        "\n",
        "        generated_tag_list[i-1] = current_best_tag\n",
        "    \n",
        "    return generated_tag_list\n",
        "\n",
        "\n",
        "def write_to_predicted_file_part2(predicted_file, words_list, tags_list):\n",
        "    assert len(words_list) == len(tags_list)\n",
        "\n",
        "    with open(predicted_file, \"w\", encoding=\"utf8\") as f:\n",
        "        for words, tags in zip(words_list, tags_list):  # Unpack all sentences and list of tags\n",
        "            assert len(words) == len(tags)\n",
        "            for word, tag in zip (words, tags):  # Unpack all words and tags\n",
        "                f.write(f\"{word} {tag}\\n\")\n",
        "            f.write(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {},
      "outputs": [],
      "source": [
        "# word = ['disfrutemos', 'de', 'una', 'buenísima', 'calidad', 'en', 'el', '.']\n",
        "# word = ES_train_words[0]\n",
        "# start_viterbi(word, ES_tags_unique, ES_emission_parameters, ES_transition_parameters)\n",
        "# print(f'viterbi_values: {viterbi_values}')\n",
        "# ES_generated_tag_list = generate_predictions_viterbi(word, ES_tags_unique, ES_emission_parameters, ES_transition_parameters)\n",
        "# print(ES_generated_tag_list)\n",
        "\n",
        "# Run and output Viterbi for ES\n",
        "ES_predicted_tags_list = []\n",
        "for word in ES_test_words:\n",
        "    viterbi_values = {}\n",
        "    start_viterbi(word, ES_unique_words, ES_tags_unique, ES_emission_parameters, ES_transition_parameters)\n",
        "    ES_generated_tag_list = generate_predictions_viterbi(word, ES_unique_words, ES_tags_unique, ES_emission_parameters, ES_transition_parameters)\n",
        "    ES_predicted_tags_list.append(ES_generated_tag_list)\n",
        "\n",
        "write_to_predicted_file_part2(ES_PART2_PREDICTED_DATA_FILE, ES_test_words, ES_predicted_tags_list)\n",
        "\n",
        "\n",
        "# Run and output Viterbi for RU\n",
        "RU_predicted_tags_list = []\n",
        "for word in RU_test_words:\n",
        "    viterbi_values = {}\n",
        "    start_viterbi(word, RU_unique_words, RU_tags_unique, RU_emission_parameters, RU_transition_parameters)\n",
        "    RU_generated_tag_list = generate_predictions_viterbi(word, RU_unique_words, RU_tags_unique, RU_emission_parameters, RU_transition_parameters)\n",
        "    RU_predicted_tags_list.append(RU_generated_tag_list)\n",
        "\n",
        "write_to_predicted_file_part2(RU_PART2_PREDICTED_DATA_FILE, RU_test_words, RU_predicted_tags_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "#Entity in gold data: 255\n",
            "#Entity in prediction: 551\n",
            "\n",
            "#Correct Entity : 131\n",
            "Entity  precision: 0.2377\n",
            "Entity  recall: 0.5137\n",
            "Entity  F: 0.3251\n",
            "\n",
            "#Correct Sentiment : 104\n",
            "Sentiment  precision: 0.1887\n",
            "Sentiment  recall: 0.4078\n",
            "Sentiment  F: 0.2581\n"
          ]
        }
      ],
      "source": [
        "from EvalScript.evalResult import get_observed, get_predicted,compare_observed_to_predicted\n",
        "\n",
        "def evaluateScores(actual_file, predicted_file):\n",
        "    with open(predicted_file, encoding=\"utf8\") as f:\n",
        "        predicted = f.read().splitlines()\n",
        "\n",
        "    with open(actual_file, encoding=\"utf8\") as f:\n",
        "        actual = f.read().splitlines()\n",
        "   \n",
        "    compare_observed_to_predicted(get_observed(actual), get_predicted(predicted))\n",
        "\n",
        "evaluateScores(ES_ACTUAL_DATA_FILE, ES_PART2_PREDICTED_DATA_FILE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "part1.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "6bea1edf50713343d797f2d3f5675b7d568eeccbe5314af310a951584ccf1264"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit ('data-sci': virtualenv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
