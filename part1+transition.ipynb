{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "x_f4Mywc7Wav"
      },
      "outputs": [],
      "source": [
        "es_train_data_file = \"./ES/train\"\n",
        "es_test_data_file = \"./ES/dev.in\"\n",
        "es_predicted_data_file = \"./ES/dev.p1.out\"\n",
        "es_actual_data_file = \"./ES/dev.out\"\n",
        "ru_train_data_file = \"./RU/train\"\n",
        "ru_test_data_file = \"./RU/dev.in\"\n",
        "ru_predicted_data_file = \"./RU/dev.p1.out\"\n",
        "ru_actual_data_file = \"./RU/dev.out\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1 - Emission Paramters, UNK Token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "lQg0WHpM7Wa0"
      },
      "outputs": [],
      "source": [
        "def _preprocess_training_file(file_path):\n",
        "    with open(file_path, encoding=\"utf8\") as f_lines:\n",
        "        data = f_lines.read().splitlines()\n",
        "        data[:] = [x for x in data if x]\n",
        "\n",
        "    output = []\n",
        "    for i in data:\n",
        "        i = i.split(\" \")\n",
        "        if(len(i) > 2):\n",
        "            i = [\" \".join(i[0:len(i)-1]), i[len(i)-1]]\n",
        "            output.append(i)\n",
        "        else:\n",
        "            output.append(i)\n",
        "        \n",
        "    return output\n",
        "\n",
        "def _preprocess_testing_file(path):\n",
        "    with open(path, encoding=\"utf8\") as f:\n",
        "        data = f.read().splitlines()\n",
        "\n",
        "    output = []\n",
        "    for word in data:\n",
        "        # if word: # leave the newlines or not?\n",
        "        output.append(word)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "EoDYOryl7Wa1"
      },
      "outputs": [],
      "source": [
        "def get_emission_using_MLE(training, k = 1):\n",
        "    tags = {}\n",
        "    tags_to_word = {}\n",
        "    emission = {}\n",
        "    for data in training:\n",
        "        word, tag = data[0], data[1]\n",
        "        if tag in tags:\n",
        "            tags[tag] += 1\n",
        "        else:\n",
        "            tags[tag] = 1\n",
        "        \n",
        "        tag_to_word = tuple((tag, word))\n",
        "        if tag_to_word in tags_to_word:\n",
        "            tags_to_word[tag_to_word] += 1\n",
        "        else:\n",
        "            tags_to_word[tag_to_word] = 1\n",
        "\n",
        "    for key in tags_to_word.keys():\n",
        "        emission[key] = tags_to_word[key] / (tags[key[0]] + k)\n",
        "    for key in tags.keys():\n",
        "        transition = tuple((key, \"#UNK#\"))\n",
        "        emission[transition] = k / (tags[key] + k)\n",
        "    # print(emission)\n",
        "    return emission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "p4FTr7EO7Wa2"
      },
      "outputs": [],
      "source": [
        "def get_most_probable_tag(emission):\n",
        "    highest_prob = {}\n",
        "    output = {}\n",
        "    for key, prob in emission.items():\n",
        "        tag, word = key[0], key[1]\n",
        "        if word not in highest_prob:\n",
        "            highest_prob[word] = prob\n",
        "            output[word] = tag\n",
        "        else:\n",
        "            if prob > highest_prob[word]:\n",
        "                highest_prob[word] = prob\n",
        "                output[word] = tag\n",
        "    # print(output)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "d4wyxNfF7Wa2"
      },
      "outputs": [],
      "source": [
        "def get_predicted_file(predicted_file, test_file, most_probable_tag):\n",
        "    f = open(predicted_file, \"w\", encoding=\"utf8\")\n",
        "    for word in test_file:\n",
        "        if len(word) > 0:\n",
        "            try:\n",
        "                y = most_probable_tag[word]\n",
        "            except:\n",
        "                y = most_probable_tag[\"#UNK#\"]\n",
        "            f.write(f\"{word} {y}\\n\")\n",
        "        else: # leave the newlines??\n",
        "            f.write(\"\\n\")\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "jXeN5YOL7Wa3"
      },
      "outputs": [],
      "source": [
        "ES_train_data = _preprocess_training_file(es_train_data_file)\n",
        "ES_test_data = _preprocess_testing_file(es_test_data_file)\n",
        "\n",
        "emission_parameters = get_emission_using_MLE(ES_train_data)\n",
        "most_probable_tag = get_most_probable_tag(emission_parameters)\n",
        "get_predicted_file(es_predicted_data_file, ES_test_data, most_probable_tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "qKJsLOOC7Wa4",
        "outputId": "4d84d753-2a74-4f45-a3bc-09891a7451a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6397645271553362"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def evaluate_precision(path_predicted, path_actual):\n",
        "    correctly_predicted_entities = 0\n",
        "    predicted_entities = 0\n",
        "    with open(path_predicted, encoding=\"utf8\") as f:\n",
        "        predicted_data = f.read().splitlines()\n",
        "        predicted_data[:] = [x for x in predicted_data if x]\n",
        "\n",
        "    with open(path_actual, encoding=\"utf8\") as f:\n",
        "        actual_data = f.read().splitlines()\n",
        "        actual_data[:] = [x for x in actual_data if x]\n",
        "\n",
        "    for i in range(len(predicted_data)):\n",
        "        predicted_entities += 1\n",
        "        if predicted_data[i] == actual_data[i]:\n",
        "            correctly_predicted_entities += 1\n",
        "\n",
        "    return correctly_predicted_entities / predicted_entities\n",
        "\n",
        "evaluate_precision(es_predicted_data_file, es_actual_data_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2i - Transition Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Ts1g5HEZoQTh"
      },
      "outputs": [],
      "source": [
        "def _preprocess_training_file(training_file):\n",
        "    tags = []\n",
        "    tags_with_start_stop = []\n",
        "    words = []    \n",
        "\n",
        "    with open(training_file, encoding=\"utf8\") as f:\n",
        "        document = f.read().rstrip()\n",
        "        lines = document.split(\"\\n\\n\")\n",
        "\n",
        "        for line in lines:\n",
        "            words_ls = []\n",
        "            tag_ls = []\n",
        "            tags_with_start_stop = []\n",
        "            for word_tag in line.split(\"\\n\"):\n",
        "                word, tag = word_tag.split(\" \")\n",
        "                words_ls.append(word)\n",
        "                tags_ls.append(tag)\n",
        "\n",
        "            tag_ls_with_start_stop = [\"START\"] + tag_ls + [\"STOP\"]\n",
        "            tags.append(tag_seq)\n",
        "            tags_with_start_stop.append(tag_ls_with_start_stop)\n",
        "            words.append(word_ls)\n",
        "    return tags, tags_with_start_stop, words\n",
        "\n",
        "def _preprocess_test_file(training_file):\n",
        "    test_words = []\n",
        "\n",
        "    with open(testing_file, encoding=\"utf8\") as f:\n",
        "        document = f.read().rstrip()\n",
        "        lines = document.split(\"\\n\\n\")\n",
        "\n",
        "        for line in lines:\n",
        "            word_ls = []\n",
        "            for word in line.split(\"\\n\"):\n",
        "                word_ls.append(word)\n",
        "            test_words.append(word_ls)\n",
        "\n",
        "    return test_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "6UGSa6o8oSq-"
      },
      "outputs": [],
      "source": [
        "def get_unique_tags(tags):\n",
        "    tags_unique = get_unique_elements(tags)\n",
        "    tags_unique.sort()\n",
        "    tags_unique_with_start_stop = [\"START\"] + tags_unique + [\"STOP\"]\n",
        "    return tags_unique, tags_unique_with_start_stop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "jiPUAxu8oWaC"
      },
      "outputs": [],
      "source": [
        "def get_transition_pairs(tags):\n",
        "    transition_pairs = []\n",
        "\n",
        "    for tag in tags:\n",
        "        #yi-1 and yi tuples\n",
        "        for tag1, tag2 in zip(tags[:-1], tags[1:]):\n",
        "            transition_pairs.append([tag1, tag2])\n",
        "\n",
        "    return transition_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "oFuy2BmcoZTC"
      },
      "outputs": [],
      "source": [
        "def count_y(tag, tags):\n",
        "    tags_flattened = list(itertools.chain.from_iterable(tags))\n",
        "    return tags_flattened.count(tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "XtJUBQ3uofA7"
      },
      "outputs": [],
      "source": [
        "def get_transition_using_MLE(tags_unique_with_start_stop, transition_pairs,\n",
        "                          tags_with_start_stop):\n",
        "  \n",
        "    transition = {}\n",
        "    #create matrix dimensions\n",
        "    for tag1 in tags_unique_with_start_stop[:-1]:\n",
        "        transition_row = {}\n",
        "        for tag2 in tags_unique_with_start_stop[1:]:\n",
        "            transition_row[tag2] = 0.0\n",
        "        transition[tag1] = transition_row\n",
        "\n",
        "    # populate transition parameters with counts\n",
        "    for tag1, tag2 in transition_pairs:\n",
        "        transition[tag1][tag2] += 1\n",
        "\n",
        "    # divide transition_count by count_yi, to get probability\n",
        "    for tag1, transition_row in transition.items():\n",
        "        count_yi = count_y(tag1, tags_with_start_stop)\n",
        "\n",
        "        # words in training set\n",
        "        for tag2, transition_count in transition_row.items():\n",
        "            transition[tag1][tag2] = transition_count / count_yi\n",
        "    \n",
        "    print(transition)\n",
        "    return transition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: For Sath to include code to run the transition parameter functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2ii - Viterbi Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Song Gee to write algorithm for Viterbi"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "part1.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "6bea1edf50713343d797f2d3f5675b7d568eeccbe5314af310a951584ccf1264"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit ('data-sci': virtualenv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
